{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14046e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.TagBert import TagBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "423e034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba32598",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='datasets'\n",
    "fname='meta_list.json'\n",
    "path=os.path.join(data_dir,fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb90420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(path)\n",
    "df=df[df['has_lyric']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4dc1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db084235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe79e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenizer,text):\n",
    "    if isinstance(text,str):\n",
    "        if text:\n",
    "            text=[text]\n",
    "    ret=[]\n",
    "    for ele in text:\n",
    "        ele=tokenizer.cls_token+ele+tokenizer.eos_token\n",
    "        ele=tokenizer.encode(ele)\n",
    "        ret.append(ele)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3257062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(model,encoded,device):\n",
    "    if not isinstance(encoded,list):\n",
    "        encoded=[encoded]\n",
    "    ret=[]\n",
    "    for ele in encoded:\n",
    "        if len(ele) >512: \n",
    "            ele=ele[:512]\n",
    "        token_ids=torch.tensor(ele,device=device).unsqueeze(0)\n",
    "        output=model(token_ids)[0]\n",
    "        output=output[:,0,:]\n",
    "        ret.append(output)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde2b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu cuda\n"
     ]
    }
   ],
   "source": [
    "tagbert=TagBert(model,tokenizer,encode,embed,device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eddcc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import DataUtils\n",
    "from translator.Papago import Papago\n",
    "\n",
    "client_id='A4Mceclx8PV864nLzpJA'\n",
    "client_secret='gSGhYL3Roj'\n",
    "\n",
    "translator=Papago(client_id,client_secret)\n",
    "utils=DataUtils(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48432ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_tags=['NNB','NNM','VXV','VXA','VCP','VCN','MAG','MAC']\n",
    "included_tags=['NNG','NNP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e6c9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "밤편지\n",
      "['마음속', '나의', '행운', '입맞춤', '사랑', '그날', '꺼', '반딧불', '눈', '위', '말']\n",
      "모든 날, 모든 순간 (Every day, Every Moment)\n",
      "['품속', '나의', '날', '유일한', '휴식처', '햇살', '하루']\n",
      "첫눈처럼 너에게 가겠다\n",
      "['숨결', '부', '순간', '질투', '축복', '빗물', '바람', '행복']\n",
      "벚꽃 엔딩\n",
      "['벚꽃', '잎', '자장 노래', '밤', '봄바람', '모습', '거리', '둘', '오예', '친구', '연인']\n",
      "봄날\n",
      "['영원', '원망', '말로', '반대편', '시간', '허공', '봄날', '그리움']\n"
     ]
    }
   ],
   "source": [
    "for title,lyric in zip(df['track_title'].iloc[:5],df['lyric'].iloc[:5]):\n",
    "    if utils.translator.get_language_type(lyric) == 'en': continue\n",
    "    tags=tagbert.extract_tag(lyric,keyphrase_ngram_range=(1, 1), stop_words=None,top_n=20)\n",
    "    tags=tags[0]\n",
    "    words=list(map(lambda x:x[0],tags))\n",
    "    probs=list(map(lambda x:x[1],tags))\n",
    "    words=utils.extractMorph(words,included_tags)\n",
    "    print(title)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dda0dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "밤편지\n",
      "['말', '반딧불', '파도', '글씨', '사랑 그날', '마음속', '입맞춤', '마음속', '그날 반딧불', '행운', '나의', '눈', '행운']\n",
      "모든 날, 모든 순간 (Every day, Every Moment)\n",
      "['눈물', '고단 나의', '미래 품속', '품속', '불안 나의', '품', '고단', '품속', '나의', '유일 휴식처', '날 햇살', '날', '눈빛 꿈', '눈물', '그날', '꿈']\n",
      "첫눈처럼 너에게 가겠다\n",
      "['삶', '부', '침', '숨결', '빗물', '한번쯤 행복', '숨결', '한번', '질투', '숨결']\n",
      "벚꽃 엔딩\n",
      "['자장 노래', '거리', '거리', '둘', '봄바람', '불면', '거리 둘', '봄바람', '둘', '거리 밤', '거리', '모습', '친구', '가요 오예', '오예 사랑', '벚꽃 잎']\n",
      "봄날\n",
      "['영원', '계절 영원', '그리움', '연기 말로', '눈꽃', '눈꽃', '봄날', '벚꽃', '그리움', '허공', '봄날', '사진', '겨울']\n"
     ]
    }
   ],
   "source": [
    "for title,lyric in zip(df['track_title'].iloc[:5],df['lyric'].iloc[:5]):\n",
    "    if utils.translator.get_language_type(lyric) == 'en': continue\n",
    "    tags=tagbert.extract_tag(lyric,keyphrase_ngram_range=(1, 2), stop_words=None,top_n=20)\n",
    "    tags=tags[0]\n",
    "    words=list(map(lambda x:x[0],tags))\n",
    "    probs=list(map(lambda x:x[1],tags))\n",
    "    words=utils.extractMorph(words,included_tags)\n",
    "    print(title)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590327e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa70e8244f78147906490339273e0c4e90a4762fa1bea59b06223bde99053082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
